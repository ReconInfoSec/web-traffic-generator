MAX_DEPTH = 10  # maximum click depth
MIN_DEPTH = 5 # minimum click depth
MAX_WAIT = 6  # maximum amount of time to wait between HTTP requests
MIN_WAIT = 2  # minimum amount of time allowed between HTTP requests
DEBUG = False  # set to True to enable useful console output

# use this single item list to test how a site responds to this crawler
# be sure to comment out the list below it.
#ROOT_URLS = ["https:///digg.com/"]

ROOT_URLS = [
	"https://digg.com/",
	"https://www.yahoo.com",
	"https://www.reddit.com",
	"http://www.cnn.com",
	"http://www.ebay.com",
	"https://en.wikipedia.org/wiki/Main_Page",
	"https://austin.craigslist.org/"
	"www.office.com",
	"www.zoom.us",
	"www.box.com",
	"www.dropbox.com",
	"www.salesforce.com",
	"www.servicenow.com",
	"https://www.atlassian.com/",
	"17ebook.co",
	"aladel.net",
	"bpwhamburgorchardpark.org",
	"clicnews.com",
	"dfwdiesel.net",
	"divineenterprises.net",
	"fantasticfilms.ru",
	"gardensrestaurantandcatering.com",
	"ginedis.com",
	"gncr.org",
	"hdvideoforums.org",
	"hihanin.com",
	"kingfamilyphotoalbum.com",
	"likaraoke.com",
	"mactep.org",
	"magic4you.nu",
	"marbling.pe.kr",
	"nacjalneg.info",
	"pronline.ru",
	"purplehoodie.com",
	"qsng.cn",
	"seksburada.net",
	"sportsmansclub.net",
	"stock888.cn",
	"tathli.com",
	"teamclouds.com",
	"texaswhitetailfever.com",
	"wadefamilytree.org",
	"xnescat.info",
	"yt118.com"
	]


# items can be a URL "https://t.co" or simple string to check for "amazon"
blacklist = [
	"https://t.co", 
	"t.umblr.com", 
	"messenger.com", 
	"itunes.apple.com", 
	"l.facebook.com", 
	"bit.ly", 
	"mediawiki", 
	".css", 
	".ico", 
	".xml", 
	"intent/tweet", 
	"twitter.com/share", 
	"signup", 
	"login", 
	"dialog/feed?", 
	".png", 
	".jpg", 
	".json", 
	".svg", 
	".gif", 
	"zendesk",
	"clickserve"
	]  

# must use a valid user agent or sites will hate you
USER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) ' \
	'AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36'
